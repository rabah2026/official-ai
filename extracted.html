<p data-block-key="3ir91"><i>Updated February 18, 2026</i></p><p data-block-key="9jteg"></p><p data-block-key="bjsft">2025 marked a major shift for AI as it became a helpful, proactive partner, capable of reasoning and navigating the world. As models grow even more sophisticated, people and businesses around the globe are transitioning from exploration to integration and finding new ways to put these tools to work in their daily lives. The transformational potential of AI is coming more clearly into focus, from foundational advances in <a href="https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/">scientific discovery</a> and <a href="https://health.google/ai-models/">clinical milestones in healthcare</a> to the rise of agentic systems capable of dramatically boosting a person’s productivity.</p><p data-block-key="410hm">Today we are sharing our latest <a href="https://ai.google/static/documents/ai-responsibility-update-2026.pdf">Responsible AI Progress Report</a>. Since we started publishing these reports, our approach to responsible AI development has continued to mature and is now fully embedded within our product development and research lifecycles. In 2025, as models become more capable, personalized and multimodal, we relied upon robust processes for testing and mitigating risks, and deepened the rigorous safeguards built into our products. To meet this challenge at the speed and scale of Google, we have paired twenty-five years of user trust insights with a comprehensive testing strategy that is driven by human expertise and supported by AI-enabled automation.</p><p data-block-key="e1ss4">Our <a href="https://ai.google/principles/">AI Principles</a> are the north star standards that guide our research, product development and business decisions. Our latest report details how we are operationalizing these principles through a multi-layered governance approach that spans the entire AI lifecycle — from initial research and model development to post-launch monitoring and remediation. The report also shows how our systems are built to be able to detect and then adapt to emerging risks in a dynamic environment.</p><p data-block-key="8n7rq">Responsibility is not only about stopping bad outputs. It is also about enabling broad access to these tools for the maximum benefit of people and society. By striking the right balance, we can ensure that AI is used to tackle major societal challenges that were previously insurmountable, from <a href="https://sites.research.google/gr/floodforecasting/">forecasting floods for 700 million people</a> to <a href="https://deepmind.google/blog/alphagenome-ai-for-better-understanding-the-genome/">decoding the human genome</a> and <a href="https://blog.google/company-news/inside-google/around-the-globe/google-asia/arda-diabetic-retinopathy-india-thailand/">helping prevent blindness</a>.</p><p data-block-key="evpql">Building trust in these tools requires deep partnership with governments, academics and civil society. As technology evolves, we remain committed to setting industry standards and sharing our research and tools with the broader ecosystem to promote uses of AI that will improve lives everywhere.</p>